1. 
	Implementation Overview:
	This program will take an 'airport file' as input and generate a model that can then predict, using the characteristics of a future flight, the estimated delay.

	My implementation is written in Python mainly because it's a fantastic prototyping language that let me make changes quickly. Although I wrote a 'from scratch' backpropagation implementation, I opted to use PyBrain since it let me quickly test out a variety of different network topologies.
	
	Now, if there's one thing I've learned from this project it's that the data is extremely detailed but disorganized. Ultimately, I decided that to get meaningful results I'd want to work on an airport by airport basis. This actually makes the whole process fairly mapreduceable since each node will deal with a given airport.

	In terms of data... I have included a data file for O'Hare (ORD) including everything necessary for training the network on 2012 data. For the final product I'll go back to 2005 or so (I'll go into why in a bit). Basically, the data is organized by flight. For a given flight, I have the temperature (icing causes delays), visibility (which impede ground operations and slow everything down even if most commercial flights operate in IFR [instrument flight rules]), wind speed, the relative humidity, the number of total operations out of the control tower (landings and takeoffs) for the day, the flight's distance according to the last filed flight plan, and the actual delay for the flight. Note that this data has been put together shoving it in a variety of different MariaDB tables and then querying out an 'airport file' with the stuff the network needs. For now, the weather data is a daily average. Some extremely involved scripting to hample irregular sampling times will allow me to use hourly estimates (and significantly improve accuracy) for the final version.

	So what does this output? Well, when you train the network it creates an airport network file. In a production system, if you wanted to check the expected delay for a flight, you could input predictions for weather information, the total number of tower operations (easy to predict with a decent non-param regression), and you could get an expected delay.

2. What I learned:
	All of the inputs are related. It was na√Øve to think I could use some sort of GLM to try and predict flight delays. I also learned that 'air operations' significantly influence the expected delay. Talking with an experienced pilot, it now makes sense why it's the case. If the tower starts falling behind due to weaher, the delays magnify since holding patterns take planes farther away from the airport. If there's simply a ton of air traffic, then planes start 'queueing' in the air and international flights get landing priority; however, planes from international flights generally aren't used right away for another flight. I also found out that "time of day" throws off most models (either that or it gets a near-zero weight) if I include it. However, time of day is very relevant if I look at whether a flight is likely to be cancelled (instead of the total delay). This result has merit: airlines are more likely to try and get the first and last flights of the day in the air in order to not throw off their future flight schedules. This is especially the case in major airports that are permitted by the FAA to have 24 hour operations. I expect that, should the FAA ATC (Air Traffic Control) furloughs happen, my model will underestimate delays if trained on 2012 or earlier data since the total number of air operations will have a larger effect. Busier days will generate significantly more delays.

3. Where now?
	I realized that the neural network part isn't all that difficult. Writing a 'from scratch' backpropagation implementation is straightforward. The trick will be finalizing the topology of the network so that I can hardcode a CUDA based solution. This is no longer a "big data" problem in that the bottleneck will ultimately be training speed. Interestingly, it will be possible to combine mapReduce with CUDA to have the "map" function train a network for each airport in a set of airports. The reduce stage will compile the resulting weights (the topologies are the same) to create a lookup table of weights for each airport. Calculating a delay for each airport then becomes a simple matter of looking up the weights for that airport and activating the network with those weights and input data. I've more or less nailed down how to generate decent training data. The query looks something like this:
		SELECT
		OnTime.Distance,Operations.Operations,DailyWeather.Visibility,DailyWeather.DryBulbCelsius,DailyWeather.StationPressure,DailyWeather.WindSpeed,OnTime.Delay
		
		FROM OnTime
		JOIN DailyWeather
			On OnTime.Date = DailyWeather.Date
		JOIN Operations
			On OnTime.Date = Operations.Date
		WHERE OnTime.Origin = <airportname>
		LIMIT <trainingsetsize>

	It's thus very easy to write a python script that will automate training and test data generation for a list of airports.

4. Challenges of Working With The Dataset:
	The data is very disparrate and in a bunch of different formats (ranging from HTML only tables to excel to csv) with inconsistent naming schemes. Ultimately, I decided that throwing everything into a MariaDB database and querying out cleaned up data was the best way to go (the database does the job). I'm going to try and write some scripts to automate the process.

	I also realized that although it would be fun to use all the data ranging back to 1985, the model's predictive ability drops astronomically when I add anything before 2005 or so. Admittedly this makes sense... technological changes have improved delays with respect to total air operations. Especially improved ILS (instrument landing systems) that allow air operations to continue in terrible visibility.
	
	Since the network trains decently with smaller subsets (1k-5k samples), I realized that assuming I can use hourly weather data, the key to optaining accuracy with decent performance will be to speed up training as much as possible. Effectively, this then becomes a processor-intensive and not data-intensive problem.